import re
from pathlib import Path
from statistics import mean
from typing import Dict, List

import spacy
import stanza  # type: ignore
from transformers import AutoModelForTokenClassification, AutoTokenizer, pipeline

from parser.entities import NEREntities


class HerbertNERClient:
    _pipeline = None

    def __init__(self):
        self.model_checkpoint = "pczarnik/herbert-base-ner"
        self.model_dir = Path("models") / "herbert-base-ner"

    def _get_pipeline(self):
        if self._pipeline is None:
            if not Path(self.model_dir).is_dir():
                print(f"Loading model from {self.model_checkpoint}...")
                tokenizer = AutoTokenizer.from_pretrained(self.model_checkpoint)
                model = AutoModelForTokenClassification.from_pretrained(
                    self.model_checkpoint
                )  # noqa: E501
                tokenizer.save_pretrained(self.model_dir)
                model.save_pretrained(self.model_dir)
            else:
                print(f"Loading model from {self.model_dir}...")
                tokenizer = AutoTokenizer.from_pretrained(self.model_dir)
                model = AutoModelForTokenClassification.from_pretrained(self.model_dir)

            HerbertNERClient._pipeline = pipeline(
                "ner", model=model, tokenizer=tokenizer
            )
            print("Model has been loaded")

        return HerbertNERClient._pipeline

    def extract_raw_entities(self, text: str) -> List[dict]:
        """Extract all entities from given text"""

        ner_pipeline = self._get_pipeline()
        return ner_pipeline(text)

    def group_entities(self, ner_output: List[dict]) -> dict:
        """Group NERs withing three categories: PER, LOC, ORG"""

        entities: Dict[str, List[dict]] = {"PER": [], "LOC": [], "ORG": []}

        current_entity: list[str] = []
        current_type = None
        current_entity_score = []

        for token in ner_output:
            tag = token["entity"]
            word = token["word"].replace("</w>", " ")
            score = float(token["score"])

            if tag.startswith("B-"):
                if current_entity and current_type:
                    entities[current_type].append(
                        {
                            "entity": "".join(current_entity),
                            "score": mean(current_entity_score),
                        }
                    )
                current_type = tag[2:]
                current_entity = [word]

            elif tag.startswith("I-") and current_type == tag[2:]:
                current_entity.append(word)
                current_entity_score.append(score)

            else:
                if current_entity and current_type:
                    entities[current_type].append(
                        {
                            "entity": "".join(current_entity),
                            "score": mean(current_entity_score),
                        }
                    )
                current_entity = []
                current_type = None

        if current_entity and current_type:
            entities[current_type].append(
                {"entity": "".join(current_entity), "score": mean(current_entity_score)}
            )

        return entities

    def fix_spacing_full_names(self, full_name: str) -> str:
        """Remove unnecessary spacing"""

        # tokenize string
        tokens = re.findall(r"\b\w+\b", full_name)

        if not tokens:
            return full_name

        result = tokens[0]
        for token in tokens[1:]:
            if token[0].isupper():
                result += " " + token
            else:
                result += token

        return result.strip()

    def parse_entities(self, text) -> NEREntities:
        """Combined logic of extracting, cleaning and parsing NER entities"""
        entities_herbert = self.extract_raw_entities(text)
        grouped_entities = self.group_entities(entities_herbert)
        personalia = [
            {
                "entity": self.fix_spacing_full_names(elem["entity"]),
                "score": elem["score"],
            }
            for elem in grouped_entities["PER"]
        ]
        locations = [
            {
                "entity": self.fix_spacing_full_names(elem["entity"]),
                "score": elem["score"],
            }
            for elem in grouped_entities["LOC"]
        ]
        organizations = [
            {
                "entity": self.fix_spacing_full_names(elem["entity"]),
                "score": elem["score"],
            }
            for elem in grouped_entities["ORG"]
        ]
        return NEREntities(personalia, locations, organizations)


class StanzaNERClient:
    _nlp_stanza = None

    def __init__(self):
        self.model_dir = Path("models") / "stanza"

    def _get_model(self):
        if self._nlp_stanza is None:
            if not (Path(self.model_dir) / "pl").is_dir():
                print(
                    f"Model is downloaded from external resource to location {self.model_dir}")  # noqa: E501
                stanza.download("pl", model_dir=str(self.model_dir))
            else:
                print(f"Model already exists in location: {self.model_dir}")

            StanzaNERClient._nlp_stanza = stanza.Pipeline(
                "pl",
                processors="tokenize,ner",  # noqa: E501
                dir=str(self.model_dir),
            )
            print("Model has been loaded")

        return StanzaNERClient._nlp_stanza

    def extract_raw_entities(self, text: str):
        """Extract all entities from given text"""

        ner_model = self._get_model()
        return ner_model(text)

    def filter_entities(self, document, pos_type: str):
        """Filter entities with pos_type: persName, placeName or orgName"""

        findings = []
        current = []

        for sentence in document.sentences:
            for token in sentence.tokens:
                ner_tag = token.ner
                text = token.text

                if ner_tag == f"B-{pos_type}":
                    current.append(text)
                elif ner_tag == f"I-{pos_type}":
                    current.append(text)
                elif ner_tag == f"E-{pos_type}":
                    current.append(text)
                    full = " ".join(current)
                    findings.append(full)
                    current = []
                elif ner_tag == f"S-{pos_type}":
                    findings.append(text)

        return findings

    def parse_entities(self, text) -> NEREntities:
        """Combined logic of extracting, cleaning and parsing NER entities"""
        stanza_entities = self.extract_raw_entities(text)
        personalia = [
            {"entity": elem, "score": 1.0}
            for elem in self.filter_entities(stanza_entities, "persName")
        ]
        locations = [
            {"entity": elem, "score": 1.0}
            for elem in self.filter_entities(stanza_entities, "placeName")
        ]
        organizations = [
            {"entity": elem, "score": 1.0}
            for elem in self.filter_entities(stanza_entities, "orgName")
        ]
        return NEREntities(personalia, locations, organizations)


class SpacyUtils:
    _nlp_spacy = None

    def _get_nlp_spacy(self):
        if self._nlp_spacy is None:
            print("Loading spacy NLP...")
            SpacyUtils._nlp_spacy = spacy.load("pl_core_news_lg")

        return SpacyUtils._nlp_spacy

    def lemmatize_name(self, name: str) -> str:
        """Return the basic form of given name"""

        nlp_spacy = self._get_nlp_spacy()
        doc = nlp_spacy(name)
        lemmatized = [token.lemma_ for token in doc]
        return " ".join(lemmatized)

    def lemmatize_names(self, names: List[str]) -> List[str]:
        """Return the basic forms of a list of names"""

        nlp_spacy = self._get_nlp_spacy()
        docs = nlp_spacy.pipe(names)
        lemmatized = []
        for doc in docs:
            lemmatized.append(" ".join([token.lemma_ for token in doc]))

        return lemmatized
    
    def count_syllables_pl(self, word: str) -> int:
        word = word.lower()
        pl_vowels = "aeiouyąęó"
        count = 0
        
        for i in range(len(word)):
            if word[i] in pl_vowels:
                # i before other pl_vowel
                if word[i] == 'i' and i + 1 < len(word) and word[i+1] in pl_vowels:
                    continue
                count += 1
                
        return max(1, count)
    
    
    
    
    def calculate_text_readability_fog(self, text: str):
        if not text or len(text.strip()) == 0:
            return 0
        
        nlp_spacy = self._get_nlp_spacy()
        doc = nlp_spacy(text)
        
        sentences = list(doc.sents)
        words = [token.text for token in doc if token.is_alpha]
        
        if not sentences or not words:
            return 0

        # hard words with minumum 4 syllabes
        hard_words = [w for w in words if self.count_syllables_pl(w) >= 4]
        
        avg_sentence_length = len(words) / len(sentences)
        percentage_hard_words = (len(hard_words) / len(words)) * 100
        
        fog_index = 0.4 * (avg_sentence_length + percentage_hard_words)
        
        return round(fog_index, 2)
    

    def texts_readability_fog(self, texts: list[str], batch_size: int = 100) -> list[float]:
        if not texts:
            return []

        nlp_spacy = self._get_nlp_spacy()
        
        # Optymalizacja: wyłączamy NER i inne zbędne moduły, jeśli potrzebujemy tylko zdań i słów.
        # To znacznie przyspiesza analizę.
        pipe_params = {
            "batch_size": batch_size,
            "disable": ["ner", "lemmatizer", "textcat"] # zostawiamy parser/senter dla doc.sents
        }

        results = []
        
        # Przetwarzanie batchowe
        for doc in nlp_spacy.pipe(texts, **pipe_params):
            if not doc or len(doc.text.strip()) == 0:
                results.append(0.0)
                continue
                
            sentences = list(doc.sents)
            words = [token.text for token in doc if token.is_alpha]
            
            if not sentences or not words:
                results.append(0.0)
                continue

            # Trudne słowa (minimum 4 sylaby)
            hard_words_count = sum(1 for w in words if self.count_syllables_pl(w) >= 4)
            
            avg_sentence_length = len(words) / len(sentences)
            percentage_hard_words = (hard_words_count / len(words)) * 100
            
            # Wzór na FOG Index
            # $$FOG = 0.4 \times \left( \frac{\text{words}}{\text{sentences}} + 100 \times \frac{\text{hard words}}{\text{words}} \right)$$
            fog_index = 0.4 * (avg_sentence_length + percentage_hard_words)
            
            results.append(round(fog_index, 2))
            
        return results